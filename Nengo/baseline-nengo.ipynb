{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import nengo\n",
    "import tensorflow as tf\n",
    "import nengo_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_breast_cancer()\n",
    "train_data = dataset.data\n",
    "train_labels = dataset.target\n",
    "\n",
    "test_data = dataset.data\n",
    "test_labels = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Build finished in 0:00:00\nOptimization finished in 0:00:00\nConstruction finished in 0:00:00\n"
    }
   ],
   "source": [
    "def classification_accuracy(y_true, y_pred):\n",
    "    return tf.metrics.sparse_categorical_accuracy(\n",
    "        y_true[:, -1], y_pred[:, -1])\n",
    "\n",
    "with nengo.Network(seed=0) as net:\n",
    "    # set some default parameters for the neurons that will make\n",
    "    # the training progress more smoothly\n",
    "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([100])\n",
    "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "    neuron_type = nengo.LIF(amplitude=0.01)\n",
    "\n",
    "    # this is an optimization to improve the training speed,\n",
    "    # since we won't require stateful behaviour in this example\n",
    "    nengo_dl.configure_settings(stateful=False)\n",
    "\n",
    "    inp = nengo.Node(np.zeros(30))\n",
    "    \n",
    "    x = nengo_dl.Layer(tf.keras.layers.Dense(units=100))(inp)\n",
    "    x = nengo_dl.Layer(neuron_type)(x)\n",
    "\n",
    "    out = nengo_dl.Layer(tf.keras.layers.Dense(units=2))(x)\n",
    "\n",
    "    out_p = nengo.Probe(out, label=\"out_p\")\n",
    "    out_p_filt = nengo.Probe(out, synapse=0.1, label=\"out_p_filt\")\n",
    "\n",
    "\n",
    "minibatch_size = 5\n",
    "sim = nengo_dl.Simulator(net, minibatch_size=minibatch_size)\n",
    "\n",
    "train_images = train_data[:, None, :]\n",
    "train_labels = train_labels[:, None, None]\n",
    "\n",
    "n_steps = 50\n",
    "test_images = np.tile(test_data[:, None, :], (1, n_steps, 1))\n",
    "test_labels = np.tile(test_labels[:, None, None], (1, n_steps, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Output out_p_filt missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to out_p_filt.\nWARNING:tensorflow:Output out_p_filt missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to out_p_filt.\nTrain on 565 samples\nEpoch 1/100\n565/565 [==============================] - 0s 383us/sample - loss: 0.2937 - out_p_loss: 0.2937\nEpoch 2/100\n565/565 [==============================] - 0s 313us/sample - loss: 0.2351 - out_p_loss: 0.2351\nEpoch 3/100\n565/565 [==============================] - 0s 319us/sample - loss: 0.2161 - out_p_loss: 0.2161\nEpoch 4/100\n565/565 [==============================] - 0s 390us/sample - loss: 0.2308 - out_p_loss: 0.2308\nEpoch 5/100\n565/565 [==============================] - 0s 324us/sample - loss: 0.2084 - out_p_loss: 0.2084\nEpoch 6/100\n565/565 [==============================] - 0s 272us/sample - loss: 0.2116 - out_p_loss: 0.2116\nEpoch 7/100\n565/565 [==============================] - 0s 470us/sample - loss: 0.2121 - out_p_loss: 0.2121\nEpoch 8/100\n565/565 [==============================] - 0s 417us/sample - loss: 0.2057 - out_p_loss: 0.2057\nEpoch 9/100\n565/565 [==============================] - 0s 417us/sample - loss: 0.2118 - out_p_loss: 0.2118\nEpoch 10/100\n565/565 [==============================] - 0s 431us/sample - loss: 0.2107 - out_p_loss: 0.2107\nEpoch 11/100\n565/565 [==============================] - 0s 494us/sample - loss: 0.2132 - out_p_loss: 0.2132\nEpoch 12/100\n565/565 [==============================] - 0s 323us/sample - loss: 0.2059 - out_p_loss: 0.2059\nEpoch 13/100\n565/565 [==============================] - 0s 287us/sample - loss: 0.1953 - out_p_loss: 0.1953\nEpoch 14/100\n565/565 [==============================] - 0s 289us/sample - loss: 0.2060 - out_p_loss: 0.2060\nEpoch 15/100\n565/565 [==============================] - 0s 287us/sample - loss: 0.2032 - out_p_loss: 0.2032\nEpoch 16/100\n565/565 [==============================] - 0s 284us/sample - loss: 0.1899 - out_p_loss: 0.1899\nEpoch 17/100\n565/565 [==============================] - 0s 295us/sample - loss: 0.2057 - out_p_loss: 0.2057\nEpoch 18/100\n565/565 [==============================] - 0s 302us/sample - loss: 0.2034 - out_p_loss: 0.2034\nEpoch 19/100\n565/565 [==============================] - 0s 284us/sample - loss: 0.1970 - out_p_loss: 0.1970\nEpoch 20/100\n565/565 [==============================] - 0s 315us/sample - loss: 0.1935 - out_p_loss: 0.1935\nEpoch 21/100\n565/565 [==============================] - 0s 274us/sample - loss: 0.1979 - out_p_loss: 0.1979\nEpoch 22/100\n565/565 [==============================] - 0s 384us/sample - loss: 0.2011 - out_p_loss: 0.2011\nEpoch 23/100\n565/565 [==============================] - 0s 312us/sample - loss: 0.1955 - out_p_loss: 0.1955\nEpoch 24/100\n565/565 [==============================] - 0s 366us/sample - loss: 0.1889 - out_p_loss: 0.1889\nEpoch 25/100\n565/565 [==============================] - 0s 331us/sample - loss: 0.1800 - out_p_loss: 0.1800\nEpoch 26/100\n565/565 [==============================] - 0s 307us/sample - loss: 0.2050 - out_p_loss: 0.2050\nEpoch 27/100\n565/565 [==============================] - 0s 426us/sample - loss: 0.1681 - out_p_loss: 0.1681\nEpoch 28/100\n565/565 [==============================] - 0s 390us/sample - loss: 0.1942 - out_p_loss: 0.1942\nEpoch 29/100\n565/565 [==============================] - 0s 360us/sample - loss: 0.1981 - out_p_loss: 0.1981\nEpoch 30/100\n565/565 [==============================] - 0s 359us/sample - loss: 0.2042 - out_p_loss: 0.2042\nEpoch 31/100\n565/565 [==============================] - 0s 330us/sample - loss: 0.1832 - out_p_loss: 0.1832\nEpoch 32/100\n565/565 [==============================] - 0s 406us/sample - loss: 0.1767 - out_p_loss: 0.1767\nEpoch 33/100\n565/565 [==============================] - 0s 319us/sample - loss: 0.1931 - out_p_loss: 0.1931\nEpoch 34/100\n565/565 [==============================] - 0s 353us/sample - loss: 0.1817 - out_p_loss: 0.1817\nEpoch 35/100\n565/565 [==============================] - 0s 349us/sample - loss: 0.1805 - out_p_loss: 0.1805\nEpoch 36/100\n565/565 [==============================] - 0s 285us/sample - loss: 0.1754 - out_p_loss: 0.1754\nEpoch 37/100\n565/565 [==============================] - 0s 299us/sample - loss: 0.1848 - out_p_loss: 0.1848\nEpoch 38/100\n565/565 [==============================] - 0s 324us/sample - loss: 0.1877 - out_p_loss: 0.1877\nEpoch 39/100\n565/565 [==============================] - 0s 388us/sample - loss: 0.1834 - out_p_loss: 0.1834\nEpoch 40/100\n565/565 [==============================] - 0s 374us/sample - loss: 0.1786 - out_p_loss: 0.1786\nEpoch 41/100\n565/565 [==============================] - 0s 295us/sample - loss: 0.1845 - out_p_loss: 0.1845\nEpoch 42/100\n565/565 [==============================] - 0s 330us/sample - loss: 0.1989 - out_p_loss: 0.1989\nEpoch 43/100\n565/565 [==============================] - 0s 289us/sample - loss: 0.1822 - out_p_loss: 0.1822\nEpoch 44/100\n565/565 [==============================] - 0s 306us/sample - loss: 0.1843 - out_p_loss: 0.1843\nEpoch 45/100\n565/565 [==============================] - 0s 310us/sample - loss: 0.1853 - out_p_loss: 0.1853\nEpoch 46/100\n565/565 [==============================] - 0s 363us/sample - loss: 0.1816 - out_p_loss: 0.1816\nEpoch 47/100\n565/565 [==============================] - 0s 302us/sample - loss: 0.1758 - out_p_loss: 0.1758\nEpoch 48/100\n565/565 [==============================] - 0s 286us/sample - loss: 0.1755 - out_p_loss: 0.1755\nEpoch 49/100\n565/565 [==============================] - 0s 285us/sample - loss: 0.1789 - out_p_loss: 0.1789\nEpoch 50/100\n565/565 [==============================] - 0s 356us/sample - loss: 0.1932 - out_p_loss: 0.1932\nEpoch 51/100\n565/565 [==============================] - 0s 363us/sample - loss: 0.1685 - out_p_loss: 0.1685\nEpoch 52/100\n565/565 [==============================] - 0s 349us/sample - loss: 0.1836 - out_p_loss: 0.1836\nEpoch 53/100\n565/565 [==============================] - 0s 403us/sample - loss: 0.1770 - out_p_loss: 0.1770\nEpoch 54/100\n565/565 [==============================] - 0s 395us/sample - loss: 0.1653 - out_p_loss: 0.1653\nEpoch 55/100\n565/565 [==============================] - 0s 315us/sample - loss: 0.1783 - out_p_loss: 0.1783\nEpoch 56/100\n565/565 [==============================] - 0s 304us/sample - loss: 0.1861 - out_p_loss: 0.1861\nEpoch 57/100\n565/565 [==============================] - 0s 401us/sample - loss: 0.1723 - out_p_loss: 0.1723\nEpoch 58/100\n565/565 [==============================] - 0s 320us/sample - loss: 0.1751 - out_p_loss: 0.1751\nEpoch 59/100\n565/565 [==============================] - 0s 332us/sample - loss: 0.1717 - out_p_loss: 0.1717\nEpoch 60/100\n565/565 [==============================] - 0s 327us/sample - loss: 0.1673 - out_p_loss: 0.1673\nEpoch 61/100\n565/565 [==============================] - 0s 300us/sample - loss: 0.1876 - out_p_loss: 0.1876\nEpoch 62/100\n565/565 [==============================] - 0s 322us/sample - loss: 0.1925 - out_p_loss: 0.1925\nEpoch 63/100\n565/565 [==============================] - 0s 316us/sample - loss: 0.1699 - out_p_loss: 0.1699\nEpoch 64/100\n565/565 [==============================] - 0s 348us/sample - loss: 0.1897 - out_p_loss: 0.1897\nEpoch 65/100\n565/565 [==============================] - 0s 344us/sample - loss: 0.1732 - out_p_loss: 0.1732\nEpoch 66/100\n565/565 [==============================] - 0s 310us/sample - loss: 0.1757 - out_p_loss: 0.1757\nEpoch 67/100\n565/565 [==============================] - 0s 312us/sample - loss: 0.1721 - out_p_loss: 0.1721\nEpoch 68/100\n565/565 [==============================] - 0s 304us/sample - loss: 0.1720 - out_p_loss: 0.1720\nEpoch 69/100\n565/565 [==============================] - 0s 336us/sample - loss: 0.1688 - out_p_loss: 0.1688\nEpoch 70/100\n565/565 [==============================] - 0s 310us/sample - loss: 0.1769 - out_p_loss: 0.1769\nEpoch 71/100\n565/565 [==============================] - 0s 307us/sample - loss: 0.1656 - out_p_loss: 0.1656\nEpoch 72/100\n565/565 [==============================] - 0s 328us/sample - loss: 0.1767 - out_p_loss: 0.1767\nEpoch 73/100\n565/565 [==============================] - 0s 372us/sample - loss: 0.1805 - out_p_loss: 0.1805\nEpoch 74/100\n565/565 [==============================] - 0s 364us/sample - loss: 0.1629 - out_p_loss: 0.1629\nEpoch 75/100\n565/565 [==============================] - 0s 350us/sample - loss: 0.1708 - out_p_loss: 0.1708\nEpoch 76/100\n565/565 [==============================] - 0s 294us/sample - loss: 0.1636 - out_p_loss: 0.1636\nEpoch 77/100\n565/565 [==============================] - 0s 275us/sample - loss: 0.1669 - out_p_loss: 0.1669\nEpoch 78/100\n565/565 [==============================] - 0s 294us/sample - loss: 0.1792 - out_p_loss: 0.1792\nEpoch 79/100\n565/565 [==============================] - 0s 369us/sample - loss: 0.1669 - out_p_loss: 0.1669\nEpoch 80/100\n565/565 [==============================] - 0s 317us/sample - loss: 0.1722 - out_p_loss: 0.1722\nEpoch 81/100\n565/565 [==============================] - 0s 349us/sample - loss: 0.1578 - out_p_loss: 0.1578\nEpoch 82/100\n565/565 [==============================] - 0s 335us/sample - loss: 0.1648 - out_p_loss: 0.1648\nEpoch 83/100\n565/565 [==============================] - 0s 308us/sample - loss: 0.1563 - out_p_loss: 0.1563\nEpoch 84/100\n565/565 [==============================] - 0s 281us/sample - loss: 0.1709 - out_p_loss: 0.1709\nEpoch 85/100\n565/565 [==============================] - 0s 342us/sample - loss: 0.1897 - out_p_loss: 0.1897\nEpoch 86/100\n565/565 [==============================] - 0s 394us/sample - loss: 0.1617 - out_p_loss: 0.1617\nEpoch 87/100\n565/565 [==============================] - 0s 302us/sample - loss: 0.1740 - out_p_loss: 0.1740\nEpoch 88/100\n565/565 [==============================] - 0s 399us/sample - loss: 0.1676 - out_p_loss: 0.1676\nEpoch 89/100\n565/565 [==============================] - 0s 296us/sample - loss: 0.1743 - out_p_loss: 0.1743\nEpoch 90/100\n565/565 [==============================] - 0s 284us/sample - loss: 0.1675 - out_p_loss: 0.1675\nEpoch 91/100\n565/565 [==============================] - 0s 300us/sample - loss: 0.1719 - out_p_loss: 0.1719\nEpoch 92/100\n565/565 [==============================] - 0s 296us/sample - loss: 0.1518 - out_p_loss: 0.1518\nEpoch 93/100\n565/565 [==============================] - 0s 282us/sample - loss: 0.1737 - out_p_loss: 0.1737\nEpoch 94/100\n565/565 [==============================] - 0s 290us/sample - loss: 0.1684 - out_p_loss: 0.1684\nEpoch 95/100\n565/565 [==============================] - 0s 304us/sample - loss: 0.1604 - out_p_loss: 0.1604\nEpoch 96/100\n565/565 [==============================] - 0s 299us/sample - loss: 0.1792 - out_p_loss: 0.1792\nEpoch 97/100\n565/565 [==============================] - 0s 340us/sample - loss: 0.1706 - out_p_loss: 0.1706\nEpoch 98/100\n565/565 [==============================] - 0s 299us/sample - loss: 0.1593 - out_p_loss: 0.1593\nEpoch 99/100\n565/565 [==============================] - 0s 282us/sample - loss: 0.1648 - out_p_loss: 0.1648\nEpoch 100/100\n565/565 [==============================] - 0s 274us/sample - loss: 0.1653 - out_p_loss: 0.1653\nWARNING:tensorflow:Output out_p missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to out_p.\nWARNING:tensorflow:Output out_p missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to out_p.\naccuracy after training: 0.9522123922288945\n"
    }
   ],
   "source": [
    "sim.compile(optimizer=tf.optimizers.RMSprop(0.001), loss={out_p: tf.losses.SparseCategoricalCrossentropy(from_logits=True)})\n",
    "sim.fit(train_images, {out_p: train_labels}, epochs=100)\n",
    "sim.compile(loss={out_p_filt: classification_accuracy})\n",
    "print(\"accuracy after training:\",sim.evaluate(test_images, {out_p_filt: test_labels}, verbose=0)[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitpy37conda34757eb3777e4ef18aaea4b8199c344c",
   "display_name": "Python 3.7.7 64-bit ('py37': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}