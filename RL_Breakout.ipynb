{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bindsnet.network import Network\n",
    "from bindsnet.pipeline import EnvironmentPipeline\n",
    "from bindsnet.learning import MSTDP, MSTDPET, PostPre\n",
    "from bindsnet.encoding import bernoulli\n",
    "from bindsnet.network.topology import Connection\n",
    "from bindsnet.environment import GymEnvironment\n",
    "from bindsnet.network.nodes import Input, LIFNodes\n",
    "from bindsnet.pipeline.action import select_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network.\n",
    "network = Network(dt=1.0)\n",
    "\n",
    "# Layers of neurons.\n",
    "inpt = Input(n=80 * 80, shape=[1, 1, 1, 80, 80], traces=True)\n",
    "middle = LIFNodes(n=200, traces=True)\n",
    "out = LIFNodes(n=4, refrac=0, traces=True)\n",
    "\n",
    "# Connections between layers.\n",
    "inpt_middle = Connection(source=inpt, target=middle, wmin=0, wmax=1e-1)\n",
    "middle_out = Connection(\n",
    "    source=middle,\n",
    "    target=out,\n",
    "    wmin=0,\n",
    "    wmax=1,\n",
    "    update_rule=PostPre,\n",
    "    nu=1e-1,\n",
    "    norm=0.5 * middle.n,\n",
    ")\n",
    "\n",
    "# Add all layers and connections to the network.\n",
    "network.add_layer(inpt, name=\"Input Layer\")\n",
    "network.add_layer(middle, name=\"Hidden Layer\")\n",
    "network.add_layer(out, name=\"Output Layer\")\n",
    "network.add_connection(inpt_middle, source=\"Input Layer\", target=\"Hidden Layer\")\n",
    "network.add_connection(middle_out, source=\"Hidden Layer\", target=\"Output Layer\")\n",
    "\n",
    "# Load the Breakout environment.\n",
    "environment = GymEnvironment(\"BreakoutDeterministic-v4\")\n",
    "environment.reset()\n",
    "\n",
    "# Build pipeline from specified components.\n",
    "environment_pipeline = EnvironmentPipeline(\n",
    "    network,\n",
    "    environment,\n",
    "    encoding=bernoulli,\n",
    "    action_function=select_softmax,\n",
    "    output=\"Output Layer\",\n",
    "    time=100,\n",
    "    history_length=5,\n",
    "    delta=1,\n",
    "    render_interval=1,\n",
    ")\n",
    "\n",
    "\n",
    "def run_pipeline(pipeline, episode_count):\n",
    "    plot_reward = []\n",
    "    for i in range(episode_count):\n",
    "        total_reward = 0\n",
    "        pipeline.reset_state_variables()\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            result = pipeline.env_step()\n",
    "            pipeline.step(result)\n",
    "\n",
    "            reward = result[1]\n",
    "            total_reward += reward\n",
    "\n",
    "            is_done = result[2]\n",
    "        plot_reward.append(total_reward)\n",
    "        print(f\"Episode {i} total reward:{total_reward}\")\n",
    "    return plot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Training: \")\n",
    "plot_reward_STDP = run_pipeline(environment_pipeline, episode_count=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network.\n",
    "network = Network(dt=1.0)\n",
    "\n",
    "# Layers of neurons.\n",
    "inpt = Input(n=80 * 80, shape=[1, 1, 1, 80, 80], traces=True)\n",
    "middle = LIFNodes(n=200, traces=True)\n",
    "out = LIFNodes(n=4, refrac=0, traces=True)\n",
    "\n",
    "# Connections between layers.\n",
    "inpt_middle = Connection(source=inpt, target=middle, wmin=0, wmax=1e-1)\n",
    "middle_out = Connection(\n",
    "    source=middle,\n",
    "    target=out,\n",
    "    wmin=0,\n",
    "    wmax=1,\n",
    "    update_rule=MSTDP,\n",
    "    nu=1e-1,\n",
    "    norm=0.5 * middle.n,\n",
    ")\n",
    "\n",
    "# Add all layers and connections to the network.\n",
    "network.add_layer(inpt, name=\"Input Layer\")\n",
    "network.add_layer(middle, name=\"Hidden Layer\")\n",
    "network.add_layer(out, name=\"Output Layer\")\n",
    "network.add_connection(inpt_middle, source=\"Input Layer\", target=\"Hidden Layer\")\n",
    "network.add_connection(middle_out, source=\"Hidden Layer\", target=\"Output Layer\")\n",
    "\n",
    "# Load the Breakout environment.\n",
    "environment = GymEnvironment(\"BreakoutDeterministic-v4\")\n",
    "environment.reset()\n",
    "\n",
    "# Build pipeline from specified components.\n",
    "environment_pipeline = EnvironmentPipeline(\n",
    "    network,\n",
    "    environment,\n",
    "    encoding=bernoulli,\n",
    "    action_function=select_softmax,\n",
    "    output=\"Output Layer\",\n",
    "    time=100,\n",
    "    history_length=5,\n",
    "    delta=1,\n",
    "    render_interval=1,\n",
    ")\n",
    "\n",
    "\n",
    "def run_pipeline(pipeline, episode_count):\n",
    "    plot_reward = []\n",
    "    for i in range(episode_count):\n",
    "        total_reward = 0\n",
    "        pipeline.reset_state_variables()\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            result = pipeline.env_step()\n",
    "            pipeline.step(result)\n",
    "\n",
    "            reward = result[1]\n",
    "            total_reward += reward\n",
    "\n",
    "            is_done = result[2]\n",
    "        plot_reward.append(total_reward)\n",
    "        print(f\"Episode {i} total reward:{total_reward}\")\n",
    "    return plot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Training: \")\n",
    "plot_reward_MSTDP = run_pipeline(environment_pipeline, episode_count=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network.\n",
    "network = Network(dt=1.0)\n",
    "\n",
    "# Layers of neurons.\n",
    "inpt = Input(n=80 * 80, shape=[1, 1, 1, 80, 80], traces=True)\n",
    "middle = LIFNodes(n=200, traces=True)\n",
    "out = LIFNodes(n=4, refrac=0, traces=True)\n",
    "\n",
    "# Connections between layers.\n",
    "inpt_middle = Connection(source=inpt, target=middle, wmin=0, wmax=1e-1)\n",
    "middle_out = Connection(\n",
    "    source=middle,\n",
    "    target=out,\n",
    "    wmin=0,\n",
    "    wmax=1,\n",
    "    update_rule=MSTDPET,\n",
    "    nu=1e-1,\n",
    "    norm=0.5 * middle.n,\n",
    ")\n",
    "\n",
    "# Add all layers and connections to the network.\n",
    "network.add_layer(inpt, name=\"Input Layer\")\n",
    "network.add_layer(middle, name=\"Hidden Layer\")\n",
    "network.add_layer(out, name=\"Output Layer\")\n",
    "network.add_connection(inpt_middle, source=\"Input Layer\", target=\"Hidden Layer\")\n",
    "network.add_connection(middle_out, source=\"Hidden Layer\", target=\"Output Layer\")\n",
    "\n",
    "# Load the Breakout environment.\n",
    "environment = GymEnvironment(\"BreakoutDeterministic-v4\")\n",
    "environment.reset()\n",
    "\n",
    "# Build pipeline from specified components.\n",
    "environment_pipeline = EnvironmentPipeline(\n",
    "    network,\n",
    "    environment,\n",
    "    encoding=bernoulli,\n",
    "    action_function=select_softmax,\n",
    "    output=\"Output Layer\",\n",
    "    time=100,\n",
    "    history_length=5,\n",
    "    delta=1,\n",
    "    render_interval=1,\n",
    ")\n",
    "\n",
    "\n",
    "def run_pipeline(pipeline, episode_count):\n",
    "    plot_reward = []\n",
    "    for i in range(episode_count):\n",
    "        total_reward = 0\n",
    "        pipeline.reset_state_variables()\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            result = pipeline.env_step()\n",
    "            pipeline.step(result)\n",
    "\n",
    "            reward = result[1]\n",
    "            total_reward += reward\n",
    "\n",
    "            is_done = result[2]\n",
    "        plot_reward.append(total_reward)\n",
    "        print(f\"Episode {i} total reward:{total_reward}\")\n",
    "    return plot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Training: \")\n",
    "plot_reward_MSTDPET = run_pipeline(environment_pipeline, episode_count=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "d = {'STDP': plot_reward_STDP, 'MSTDP': plot_reward_MSTDP, 'MSTDPET': plot_reward_MSTDPET}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv('RL_breakout_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(height=300)\n",
    "p.line(x = np.arange(len(plot_reward_STDP)), y = plot_reward_STDP, color='green')\n",
    "p.line(x = np.arange(len(plot_reward_MSTDP)), y = plot_reward_MSTDP, color='blue')\n",
    "p.line(x = np.arange(len(plot_reward_MSTDPET)), y = plot_reward_MSTDPET, color='red')\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bit11652f73c0384a9fb28c6e9b840d1f5d",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}