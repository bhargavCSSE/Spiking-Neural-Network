{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import argparse\n",
    "import itertools\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import pdb\n",
    "\n",
    "\n",
    "class MountainCar:\n",
    "    def __init__(self):\n",
    "        current_state = None\n",
    "\n",
    "    def transition_function(self, state, action):\n",
    "        x = state[0]\n",
    "        v = state[1]\n",
    "        v_n = v + 0.001*action - 0.0025*np.cos(3*x)\n",
    "        x_n = x + v_n\n",
    "        if x_n < -1.2:\n",
    "            x_n = -1.2\n",
    "            v_n = 0\n",
    "        elif x_n > 0.5:\n",
    "            x_n = 0.5\n",
    "            v_n = 0\n",
    "\n",
    "        if v_n > 0.07:\n",
    "            v_n = 0.07\n",
    "        elif v_n < -0.07:\n",
    "            v_n = -0.07\n",
    "\n",
    "        return [x_n, v_n]\n",
    "\n",
    "    def param_state(self, state, weights):\n",
    "        param_state = 0\n",
    "        for i in range(len(state)):\n",
    "            param_state += state[i]*weights[i]\n",
    "        return param_state\n",
    "\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(self, order, epsilon, step_size, lda, num_states=2):\n",
    "        self.num_states = num_states\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = step_size\n",
    "        self.lda = lda\n",
    "        self.mc = MountainCar()\n",
    "        self.order = order\n",
    "        self.w = {}\n",
    "        self.w[-1] = np.zeros(int(math.pow(order+1, num_states)))\n",
    "        self.w[0] = np.zeros(int(math.pow(order+1, num_states)))\n",
    "        self.w[1] = np.zeros(int(math.pow(order+1, num_states)))\n",
    "        self.combns = np.array(list(itertools.product(range(order+1), repeat=num_states)))\n",
    "        self.x_lim = [-1.2,0.5]\n",
    "        self.v_lim = [-0.07, 0.07]\n",
    "        self.actors = [SpikingActor() for i in range(20)]\n",
    "\n",
    "\n",
    "    def fourier_feature_state(self, state, method='fourier'):\n",
    "        state_norm = np.zeros(self.num_states)\n",
    "        state_norm[0] = (state[0]+self.x_lim[1])/(self.x_lim[1]-self.x_lim[0])\n",
    "        state_norm[1] = (state[1]+self.v_lim[1])/(self.v_lim[1]-self.v_lim[0])\n",
    "\n",
    "        prod_array = np.dot(self.combns, state_norm)\n",
    "        features = np.array(np.cos(np.pi*prod_array))\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "    def e_greedy_action(self, action_ind):\n",
    "        prob = (self.epsilon/3)*np.ones(3)\n",
    "        prob[action_ind] = (1 - self.epsilon) + (self.epsilon/3)\n",
    "        e_action = np.random.choice(3,1,p=prob)-1\n",
    "        return int(e_action)\n",
    "\n",
    "\n",
    "    def run_actor_critic(self, num_episodes, features='fourier'):\n",
    "        rewards = []\n",
    "        #theta = np.random.rand(self.num_states)\n",
    "        #theta = np.zeros(self.num_states)\n",
    "        theta = np.zeros(int(math.pow(self.order+1, self.num_states)))\n",
    "        w_v = np.zeros(int(math.pow(self.order+1, self.num_states)))\n",
    "        alpha = 0.01\n",
    "        beta = 0.001\n",
    "        for i in range(num_episodes):\n",
    "            #if i > 500:\n",
    "            #    self.alpha = 0.001\n",
    "            state = np.array([-0.5, 0])\n",
    "            e_theta = np.zeros_like(theta)\n",
    "            e_v = np.zeros(int(math.pow(self.order+1, self.num_states)))\n",
    "            rt = -1; gamma = 1\n",
    "            count = 0\n",
    "            sigma = 1\n",
    "            while state[0] < 0.5:\n",
    "                # Act using actor\n",
    "                fourier_state = self.fourier_feature_state(state, features)\n",
    "                state_param = np.dot(theta, fourier_state)\n",
    "\n",
    "                o_rates = []\n",
    "                for k in range(len(self.actors)):\n",
    "                    o_spikes = self.actors[k].forward(state, count)\n",
    "                    o_rates.append(o_spikes)\n",
    "                o_rates = np.array(o_rates)\n",
    "                action_rates = np.zeros(3)\n",
    "                for k in range(3):\n",
    "                    action_rates[k] = sum(o_rates[np.where(o_rates[:,k]==1),k][0])\n",
    "                action_index = np.argmax(action_rates)\n",
    "                action = self.e_greedy_action(action_index)\n",
    "\n",
    "                new_state = self.mc.transition_function(state, action)\n",
    "                new_state = np.array(new_state)\n",
    "                fourier_state = self.fourier_feature_state(state, features)\n",
    "                fourier_new_state = self.fourier_feature_state(new_state, features)\n",
    "\n",
    "                # Critic update\n",
    "                e_v = gamma*self.lda*e_v + fourier_state\n",
    "                v_s = np.dot(w_v, fourier_state)\n",
    "                v_ns = np.dot(w_v, fourier_new_state)\n",
    "                delta_t = rt + gamma*v_ns - v_s\n",
    "                w_v += alpha*delta_t*e_v\n",
    "\n",
    "                # Actor update\n",
    "                for k in range(len(self.actors)):\n",
    "                    self.actors[k].update_weights(delta_t, state, action+1)\n",
    "\n",
    "                #print(state, new_state)\n",
    "                state = new_state\n",
    "                count += 1\n",
    "                if count > 5000:\n",
    "                    break\n",
    "            #pdb.set_trace()\n",
    "            if count > 5000:\n",
    "                continue\n",
    "            #if i%20 == 0 or i == 99:\n",
    "            print(\"Reward after %s episodes: %s\" %(i, -count))\n",
    "            rewards.append(-1*count)\n",
    "        return rewards\n",
    "\n",
    "\n",
    "\n",
    "class SpikingActor():\n",
    "    def __init__(self):\n",
    "        self.inputs = 20\n",
    "        self.hidden = 5\n",
    "        self.outputs = 3\n",
    "        self.ih_weights = np.random.rand(self.hidden, self.inputs)\n",
    "        self.ih_bias = np.random.rand(self.hidden)\n",
    "        self.ho_weights = np.random.rand(self.outputs, self.hidden)\n",
    "        self.ho_bias = np.random.rand(self.outputs)\n",
    "        self.alpha = 0.1\n",
    "        self.h_spikes = np.ones(self.hidden)\n",
    "        self.o_spikes = np.ones(self.outputs)\n",
    "        self.in_spikes = np.ones(self.inputs)\n",
    "        self.hz = np.zeros(self.hidden)\n",
    "        self.oz = np.zeros(self.outputs)\n",
    "\n",
    "    def input_coding(self, state):\n",
    "        maps = list(itertools.combinations(range(int(self.inputs*0.5)), r=5))\n",
    "        state_code = -1*np.ones(self.inputs)\n",
    "        xb = int(self.inputs*0.5*(state[0] + 1.2)/2.4)\n",
    "        vb = int(self.inputs*0.5*(state[1] + 0.07)/0.14) \n",
    "        state_code[list(maps[xb])] = 1\n",
    "        state_code[list(np.array((maps[vb])) + int(self.inputs*0.5))] = 1\n",
    "        return state_code\n",
    "\n",
    "\n",
    "    def forward(self,state,count):\n",
    "        inputs = self.input_coding(state)\n",
    "        self.in_spikes = inputs\n",
    "\n",
    "        z = np.matmul(self.ih_weights, inputs) + self.ih_bias\n",
    "        pr = 1/(1 + np.exp(-2*z))\n",
    "        self.h_spikes = (pr > np.random.rand(self.hidden)).astype(int)\n",
    "        self.h_spikes = 2*self.h_spikes - 1\n",
    "        self.hz = np.exp(z) + np.exp(-z)\n",
    "\n",
    "\n",
    "        zo = np.matmul(self.ho_weights, self.h_spikes) + self.ho_bias\n",
    "        po = 1/(1 + np.exp(-2*zo + 1 ))\n",
    "        self.o_spikes = (po > np.random.rand(self.outputs)).astype(int)\n",
    "        self.o_spikes = 2*self.o_spikes - 1\n",
    "        self.oz = np.exp(zo) + np.exp(-zo)\n",
    "\n",
    "\n",
    "        return self.o_spikes\n",
    "\n",
    "    def update_weights(self, tderror, state, action):\n",
    "        #print(state, action, self.h_spikes, self.o_spikes, tderror)\n",
    "\n",
    "        h_grad = self.h_spikes\n",
    "        h_grad[np.where(self.h_spikes) == -1] = -2*self.hz\n",
    "        h_grad[np.where(self.h_spikes) == 1] = 2*(1-self.hz)\n",
    "        self.ih_bias += self.alpha*tderror*h_grad\n",
    "        self.ih_weights += self.alpha*tderror*np.outer(h_grad, self.in_spikes)\n",
    "        #pdb.set_trace()\n",
    "\n",
    "        o_grad = self.o_spikes\n",
    "        o_grad[np.where(self.o_spikes) == -1] = -2*self.oz\n",
    "        o_grad[np.where(self.o_spikes) == 1] = 2*(1-self.oz)\n",
    "\n",
    "        self.ho_bias += self.alpha*tderror*o_grad\n",
    "\n",
    "        for i in range(self.outputs):\n",
    "            if i == action:\n",
    "                for j in range(self.hidden):\n",
    "                    self.ho_weights[i,j] += self.alpha*tderror*o_grad[i]*self.h_spikes[j]\n",
    "            if i != action and tderror > 0:\n",
    "                for j in range(self.hidden):\n",
    "                    self.ho_weights[i,j] += self.alpha*tderror*o_grad[i]*self.h_spikes[j]\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "    algorithm = 'ac'\n",
    "    features = 'fourier'\n",
    "    selection = 'egreedy'\n",
    "    num_trials = 1\n",
    "    num_episodes = 1000\n",
    "    plot = True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = Args()\n",
    "\n",
    "    rewards_trials = []\n",
    "\n",
    "    step_size = 0.01\n",
    "    epsilon = 0.1\n",
    "    lda = 0.5\n",
    "\n",
    "\n",
    "    for i in range(int(args.num_trials)):\n",
    "        print('Trial:', i)\n",
    "        td_cp = ActorCritic(order=4, epsilon=epsilon, step_size=step_size, lda=lda)\n",
    "        rewards = td_cp.run_actor_critic(int(args.num_episodes), features='fourier')\n",
    "        rewards_trials.append(rewards)\n",
    "\n",
    "    print(\"Maximum reward reached at the end of 1000 episodes : \", np.mean(rewards_trials, axis=0)[-1] )\n",
    "\n",
    "    if args.plot:\n",
    "        episodes = np.linspace(0,int(args.num_episodes)-1,int(args.num_episodes))\n",
    "        rewards_mean = np.mean(rewards_trials, axis=0)\n",
    "        rewards_std = np.std(rewards_trials, axis=0)\n",
    "        plt.errorbar(episodes, rewards_mean, rewards_std)\n",
    "        plt.ylabel('Mean reward')\n",
    "        plt.xlabel('Number of episodes')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import Band, ColumnDataSource\n",
    "output_notebook()\n",
    "\n",
    "Rm = rewards_mean\n",
    "mean_rewards = []\n",
    "std_rewards = []\n",
    "for i in range(len(Rm)):\n",
    "    mean_rewards.append(np.mean(Rm[:(i+1)]))\n",
    "    std_rewards.append(np.std(Rm[:(i+1)]))\n",
    "mean_rewards = np.array(mean_rewards)\n",
    "std_rewards = np.array(std_rewards)\n",
    "\n",
    "source = ColumnDataSource(dict(\n",
    "    x = episodes,\n",
    "    mean = mean_rewards,\n",
    "    std = std_rewards,\n",
    "    upper = mean_rewards + std_rewards,\n",
    "    lower = mean_rewards - std_rewards\n",
    "))\n",
    "\n",
    "p0 = figure(\n",
    "    title = \"Rewards (Mountain car)\",\n",
    "    x_axis_label = \"Episodes\",\n",
    "    y_axis_label = \"Rewards\"\n",
    ")\n",
    "p0.line(x=episodes, y=Rm, line_width=2)\n",
    "\n",
    "p3 = figure(\n",
    "    title = \"Mean-Variance (Mountain car)\",\n",
    "    x_axis_label = \"Episodes\",\n",
    "    y_axis_label = \"Rewards\"\n",
    ")\n",
    "band = Band(base='x', lower='lower', upper='upper', source=source, level='underlay',\n",
    "            fill_alpha=0.2, line_width=1, line_color='black', fill_color='green')\n",
    "p3.line(x=episodes, y=mean_rewards, line_width=2, color='green')\n",
    "p3.add_layout(band)\n",
    "\n",
    "show(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitf046163c483e4c289e3ad1ebe4f16c22",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}