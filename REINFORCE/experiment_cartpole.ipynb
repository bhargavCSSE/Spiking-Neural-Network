{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Trial: 0\nReward after 0 episodes: 10\nReward after 1 episodes: 12\nReward after 2 episodes: 29\nReward after 3 episodes: 9\nReward after 4 episodes: 13\nReward after 5 episodes: 21\nReward after 6 episodes: 21\nReward after 7 episodes: 10\nReward after 8 episodes: 18\nReward after 9 episodes: 12\nTrial: 0\nReward after 0 episodes: 16\nReward after 1 episodes: 15\nReward after 2 episodes: 11\nReward after 3 episodes: 26\nReward after 4 episodes: 8\nReward after 5 episodes: 13\nReward after 6 episodes: 10\nReward after 7 episodes: 22\nReward after 8 episodes: 14\nReward after 9 episodes: 51\nTrial: 0\nReward after 0 episodes: 37\nReward after 1 episodes: 10\nReward after 2 episodes: 15\nReward after 3 episodes: 55\nReward after 4 episodes: 11\nReward after 5 episodes: 32\nReward after 6 episodes: 13\nReward after 7 episodes: 17\nReward after 8 episodes: 11\nReward after 9 episodes: 21\nTrial: 0\nReward after 0 episodes: 14\nReward after 1 episodes: 19\nReward after 2 episodes: 12\nReward after 3 episodes: 16\nReward after 4 episodes: 24\nReward after 5 episodes: 10\nReward after 6 episodes: 10\nReward after 7 episodes: 32\nReward after 8 episodes: 15\nReward after 9 episodes: 33\n"
    }
   ],
   "source": [
    "#-----------------------------------\n",
    "# Author: Sneha Reddy Aenugu\n",
    "# Description: Cartpole balancing\n",
    "# with spiking agent actor-critic\n",
    "#----------------------------------\n",
    "Gamma = [0.25, 0.5, 0.75, 1]\n",
    "data = {}\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import argparse\n",
    "import random\n",
    "import itertools\n",
    "import pdb\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "for gam in Gamma:\n",
    "    #Constants\n",
    "\n",
    "    mod_F = 10\n",
    "    m_c = 1\n",
    "    m_p = 0.1\n",
    "    l = 0.5\n",
    "    td = 0.02\n",
    "    g = 9.8\n",
    "\n",
    "\n",
    "    class ActorCritic():\n",
    "        def __init__(self, order, epsilon, step_size, sigma=0.1, num_states=4, radial_sigma=None):\n",
    "            self.num_states = num_states\n",
    "            self.epsilon = epsilon\n",
    "            self.alpha = step_size\n",
    "            self.sigma = sigma\n",
    "\n",
    "            self.cartpole = gym.make(\"CartPole-v0\")\n",
    "            self.order = order\n",
    "            self.lda = 0.5\n",
    "            self.w = {}\n",
    "\n",
    "            self.w[-1] = 5*np.ones(int(math.pow(order+1, num_states)))\n",
    "            self.w[1] = 5*np.ones(int(math.pow(order+1, num_states)))\n",
    "            \n",
    "            self.combns = np.array(list(itertools.product(range(order+1), repeat=num_states)))\n",
    "            self.x_lim = [-3,3]\n",
    "            self.v_lim = [-10,10]\n",
    "            self.theta_lim = [-math.pi/2,math.pi/2]\n",
    "            self.omega_lim = [-math.pi, math.pi]\n",
    "            self.actors = [SpikingActor() for i in range(10)]\n",
    "\n",
    "\n",
    "        def fourier_feature_state(self, state, method='fourier'):\n",
    "            state_norm = np.zeros(self.num_states)\n",
    "            state_norm[0] = (state[0]+self.x_lim[1])/(self.x_lim[1]-self.x_lim[0])\n",
    "            state_norm[1] = (state[1]+self.v_lim[1])/(self.v_lim[1]-self.v_lim[0])\n",
    "            state_norm[2] = (state[2]+self.theta_lim[1])/(self.theta_lim[1]-self.theta_lim[0])\n",
    "            state_norm[3] = (state[3]+self.omega_lim[1])/(self.omega_lim[1]-self.omega_lim[0])\n",
    "\n",
    "            prod_array = np.array([np.dot(state_norm, i) for i in self.combns])\n",
    "            features = np.array(np.cos(np.pi*prod_array))\n",
    "            return features\n",
    "\n",
    "\n",
    "        def e_greedy_action(self, action_ind):\n",
    "            prob = (self.epsilon/2)*np.ones(2)\n",
    "            prob[action_ind] = (1 - self.epsilon) + (self.epsilon/2)\n",
    "            #e_action = 2*np.random.choice(2,1,p=prob)-1\n",
    "            pr_array = np.concatenate((np.ones(int(100*prob[1])), -1*np.ones(int(100*prob[0]))))\n",
    "            e_action = pr_array[random.randint(0, len(pr_array)-1)]\n",
    "            return int(e_action)\n",
    "\n",
    "\n",
    "        def softmax_selection(self, qvalues, sigma):\n",
    "            eps = 1e-5\n",
    "            qvalues = qvalues + eps\n",
    "            prob = np.exp(sigma*qvalues)/sum(np.exp(sigma*qvalues))\n",
    "            prob[1] = 1-prob[0]\n",
    "            e_action = 2*np.random.choice(2,1,p=prob)-1\n",
    "            return int(e_action)\n",
    "\n",
    "\n",
    "        def run_actor_critic(self, num_episodes, features='fourier'):\n",
    "            rewards = []\n",
    "            #theta = np.random.rand(self.num_states)\n",
    "            #theta = np.zeros(self.num_states)\n",
    "            theta = np.zeros(int(math.pow(self.order+1, self.num_states)))\n",
    "            w_v = np.zeros(int(math.pow(self.order+1, self.num_states)))\n",
    "            alpha = 0.001\n",
    "            for i in range(num_episodes):\n",
    "                #if i > 500:\n",
    "                #    self.alpha = 0.001\n",
    "                #state = np.zeros(4)\n",
    "                state = self.cartpole.reset()\n",
    "                e_theta = np.zeros_like(theta)\n",
    "                e_v = np.zeros(int(math.pow(self.order+1, self.num_states)))\n",
    "                rt = 1; gamma = gam\n",
    "                count = 0\n",
    "                sigma = 1\n",
    "                while abs(state[0]) < 3 and abs(state[2]) < math.pi/2 and abs(state[3]) < math.pi and count < 1010:\n",
    "                    # Act using actor\n",
    "                    fourier_state = self.fourier_feature_state(state, features)\n",
    "                    state_param = np.dot(theta, fourier_state)\n",
    "\n",
    "                    o_rates = []\n",
    "                    for k in range(len(self.actors)):\n",
    "                        o_spikes = self.actors[k].forward(state, count)\n",
    "                        o_rates.append(o_spikes)\n",
    "                    o_rates = np.array(o_rates)\n",
    "                    action_rates = np.zeros(2)\n",
    "                    for k in range(2):\n",
    "                        action_rates[k] = sum(o_rates[np.where(o_rates[:,k]==1),k][0])\n",
    "                    action_index = np.argmax(action_rates)\n",
    "                    action = self.e_greedy_action(action_index)\n",
    "\n",
    "                    new_state, reward, done, _ = self.cartpole.step(int((action+1)/2))\n",
    "                    fourier_state = self.fourier_feature_state(state, features)\n",
    "                    fourier_new_state = self.fourier_feature_state(new_state, features)\n",
    "\n",
    "                    # Critic update\n",
    "                    e_v = gamma*self.lda*e_v + fourier_state\n",
    "                    v_s = np.dot(w_v, fourier_state)\n",
    "                    v_ns = np.dot(w_v, fourier_new_state)\n",
    "                    delta_t = rt + gamma*v_ns - v_s\n",
    "                    w_v += alpha*delta_t*e_v\n",
    "\n",
    "                    # Actor update\n",
    "\n",
    "                    for k in range(len(self.actors)):\n",
    "                        self.actors[k].update_weights(delta_t, state, int((action+1)/2), np.mean(rewards[-10:]))\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                    state = new_state\n",
    "                    count += 1\n",
    "\n",
    "                print(\"Reward after %s episodes: %s\" %(i, count))\n",
    "                rewards.append(count)\n",
    "            return rewards\n",
    "\n",
    "    class SpikingActor():\n",
    "        def __init__(self):\n",
    "            self.inputs = 4\n",
    "            self.hidden = 200\n",
    "            self.outputs = 2\n",
    "            self.ih_weights = 0.01*np.random.rand(2, self.hidden, self.inputs)\n",
    "            self.ih_bias = np.random.rand(self.hidden)\n",
    "            self.ho_weights = 0.01*np.random.rand(self.outputs, self.hidden)\n",
    "            self.ho_bias = np.random.rand(self.outputs)\n",
    "            self.alpha = 0.001\n",
    "            self.h_spikes = np.ones(self.hidden)\n",
    "            self.o_spikes = np.ones(self.outputs)\n",
    "            self.in_spikes = np.ones(self.inputs)\n",
    "            self.hz = np.zeros(self.hidden)\n",
    "            self.oz = np.zeros(self.outputs)\n",
    "\n",
    "        def input_coding(self, state):\n",
    "            maps = list(itertools.combinations(range(int(self.inputs*0.25)), r=int(self.inputs*0.25*0.5)))\n",
    "            state_code = -1*np.ones(self.inputs)\n",
    "            xb = int(self.inputs*0.25*(state[0] + 3)/6)\n",
    "            vb = int(self.inputs*0.25*(state[1] + 10)/20)\n",
    "            thetab = int(self.inputs*0.25*(state[0] + math.pi/2)/math.pi)\n",
    "            omegab = int(self.inputs*0.25*(state[1] + math.pi)/(2*math.pi))\n",
    "            state_code[list(maps[xb])] = 1\n",
    "            state_code[list(np.array((maps[vb])) + int(self.inputs*0.25))] = 1\n",
    "            state_code[list(np.array((maps[thetab])) + int(self.inputs*0.5))] = 1\n",
    "            state_code[list(np.array((maps[omegab])) + int(self.inputs*0.75))] = 1\n",
    "            return state_code\n",
    "\n",
    "\n",
    "        def forward(self,state,count):\n",
    "            inputs = state\n",
    "            self.in_spikes = state\n",
    "\n",
    "            self.hz = np.zeros((2, self.hidden))\n",
    "            self.h_spikes = np.ones((2, self.hidden))\n",
    "            for i in range(2):\n",
    "                z = np.matmul(self.ih_weights[i], inputs)\n",
    "                p = 1/(1 + np.exp(-2*z))\n",
    "                self.h_spikes[i] = (p > np.random.rand(self.hidden)).astype(int)\n",
    "                self.h_spikes[i] = 2*self.h_spikes[i] - 1\n",
    "                self.hz[i] = 1 + np.exp(2*z*self.h_spikes[i])\n",
    "\n",
    "\n",
    "            self.oz = np.zeros(self.outputs)\n",
    "            self.o_spikes = np.ones(self.outputs)\n",
    "\n",
    "            for i in range(2):\n",
    "                zo = np.dot(self.ho_weights[i], self.h_spikes[i])\n",
    "                po = 1/(1 + np.exp(-2*zo))\n",
    "                self.o_spikes[i] = (po > np.random.rand(1)).astype(int)\n",
    "                self.o_spikes[i] = 2*self.o_spikes[i] - 1\n",
    "                self.oz[i] = 1 + np.exp(2*zo*self.o_spikes[i])\n",
    "\n",
    "            return self.o_spikes\n",
    "\n",
    "        def update_weights(self, tderror, state, action, mean_reward):\n",
    "\n",
    "            if mean_reward > 70 and mean_reward < 190:\n",
    "                self.alpha = 0.00001\n",
    "            elif mean_reward > 190:\n",
    "                self.alpha = 0.00001\n",
    "            else:\n",
    "                self.alpha = 0.001\n",
    "\n",
    "            for i in range(2):\n",
    "                if i == action:\n",
    "                    self.ih_weights[i] += self.alpha*tderror*np.outer(2*self.h_spikes[i]/self.hz[i], self.in_spikes)\n",
    "                else:\n",
    "                    if self.o_spikes[i] == 1:\n",
    "                        self.ih_weights[i] -= self.alpha*tderror*np.outer(2*self.h_spikes[i]/self.hz[i], self.in_spikes)\n",
    "                    else:\n",
    "                        self.ih_weights[i] += self.alpha*tderror*np.outer(2*self.h_spikes[i]/self.hz[i], self.in_spikes)\n",
    "\n",
    "\n",
    "            for i in range(2):\n",
    "                if i == action:\n",
    "                    self.ho_weights[i] += self.alpha*tderror*np.multiply(2*self.o_spikes[i]/self.oz[i], self.h_spikes[i])\n",
    "                else:\n",
    "                    if self.o_spikes[i] == 1:\n",
    "                        self.ho_weights[i] -= self.alpha*tderror*np.multiply(2*self.o_spikes[i]/self.oz[i], self.h_spikes[i])\n",
    "                    else:\n",
    "                        self.ho_weights[i] += self.alpha*tderror*np.multiply(2*self.o_spikes[i]/self.oz[i], self.h_spikes[i])\n",
    "\n",
    "\n",
    "\n",
    "    class Args:\n",
    "        algorithm = 'sarsa'\n",
    "        features = 'fourier'\n",
    "        selection = 'egreedy'\n",
    "        num_trials = 1\n",
    "        num_episodes = 1000\n",
    "        plot = False\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "\n",
    "        args = Args()\n",
    "\n",
    "\n",
    "        rewards_trials = []\n",
    "\n",
    "\n",
    "        step_size = 0.001 # Sarsa, fourier 0.001\n",
    "        epsilon = 0.1\n",
    "        \n",
    "\n",
    "        for i in range(int(args.num_trials)):\n",
    "            print('Trial:', i)\n",
    "            td_cp = ActorCritic(order=5, epsilon=epsilon, step_size=step_size, num_states=4)\n",
    "            rewards = td_cp.run_actor_critic(int(args.num_episodes), features='fourier')\n",
    "            rewards_trials.append(rewards)\n",
    "\n",
    "\n",
    "        if args.plot:\n",
    "            episodes = np.linspace(0,int(args.num_episodes)-1,int(args.num_episodes))\n",
    "            rewards_mean = np.mean(rewards_trials, axis=0)\n",
    "            rewards_std = np.std(rewards_trials, axis=0)\n",
    "            plt.errorbar(episodes, rewards_mean, rewards_std)\n",
    "            plt.ylabel('Mean reward')\n",
    "            plt.xlabel('Number of episodes')\n",
    "            plt.show()\n",
    "\n",
    "    data[gam] = np.array(rewards_trials).reshape(-1)\n",
    "\n",
    "d = pd.DataFrame(data)\n",
    "d.to_csv(\"cartpole_gamma_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   0.25  0.50  0.75  1.00\n0    10    16    37    14\n1    12    15    10    19\n2    29    11    15    12\n3     9    26    55    16\n4    13     8    11    24\n5    21    13    32    10\n6    21    10    13    10\n7    10    22    17    32\n8    18    14    11    15\n9    12    51    21    33",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0.25</th>\n      <th>0.50</th>\n      <th>0.75</th>\n      <th>1.00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>16</td>\n      <td>37</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12</td>\n      <td>15</td>\n      <td>10</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>29</td>\n      <td>11</td>\n      <td>15</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>26</td>\n      <td>55</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13</td>\n      <td>8</td>\n      <td>11</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>21</td>\n      <td>13</td>\n      <td>32</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>21</td>\n      <td>10</td>\n      <td>13</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>22</td>\n      <td>17</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>18</td>\n      <td>14</td>\n      <td>11</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>12</td>\n      <td>51</td>\n      <td>21</td>\n      <td>33</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import Band, ColumnDataSource\n",
    "output_notebook()\n",
    "\n",
    "Rm = rewards_mean\n",
    "mean_rewards = []\n",
    "std_rewards = []\n",
    "for i in range(len(Rm)):\n",
    "    mean_rewards.append(np.mean(Rm[:(i+1)]))\n",
    "    std_rewards.append(np.std(Rm[:(i+1)]))\n",
    "mean_rewards = np.array(mean_rewards)\n",
    "std_rewards = np.array(std_rewards)\n",
    "\n",
    "source = ColumnDataSource(dict(\n",
    "    x = episodes,\n",
    "    mean = mean_rewards,\n",
    "    std = std_rewards,\n",
    "    upper = mean_rewards + std_rewards,\n",
    "    lower = mean_rewards - std_rewards\n",
    "))\n",
    "\n",
    "p0 = figure(\n",
    "    title = \"Rewards (Cartpole-v0)\",\n",
    "    x_axis_label = \"Episodes\",\n",
    "    y_axis_label = \"Rewards\"\n",
    ")\n",
    "p0.line(x=episodes, y=Rm, line_width=2)\n",
    "\n",
    "p1 = figure(\n",
    "    title = \"Mean-Variance (Cartpole-v0)\",\n",
    "    x_axis_label = \"Episodes\",\n",
    "    y_axis_label = \"Rewards\"\n",
    ")\n",
    "band = Band(base='x', lower='lower', upper='upper', source=source, level='underlay',\n",
    "            fill_alpha=0.2, line_width=1, line_color='black', fill_color='green')\n",
    "p1.line(x=episodes, y=mean_rewards, line_width=2, color='green')\n",
    "# p1.circle(x=episodes, y=mean_rewards, line_width=2, color='green')\n",
    "p1.add_layout(band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitf046163c483e4c289e3ad1ebe4f16c22",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}